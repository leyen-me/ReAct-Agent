# -*- coding: utf-8 -*-
"""ReAct Agent ä¸»é€»è¾‘"""

import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable, Tuple

from openai import OpenAI, Stream
from openai.types.chat import ChatCompletionChunk

from config import config
from tools import (
    Tool,
    ReadFileTool,
    WriteFileTool,
    DeleteFileTool,
    CreateFileTool,
    RenameFileTool,
    ListFilesTool,
    TreeFilesTool,
    EditFileTool,
    CreateFolderTool,
    DeleteFolderTool,
    MoveFileTool,
    CopyFileTool,
    RunCommandTool,
    SearchInFilesTool,
    FindFilesTool,
    GitStatusTool,
    GitDiffTool,
    GitCommitTool,
    GitBranchTool,
    GitLogTool,
)
from tool_executor import create_tool_executor
from task_planner import TaskPlanner, TaskPlan, PlanStep, StepStatus

logger = logging.getLogger(__name__)


class MessageManager:
    """æ¶ˆæ¯ç®¡ç†å™¨"""

    def __init__(self, system_prompt: str, max_context_tokens: int):
        """
        åˆå§‹åŒ–æ¶ˆæ¯ç®¡ç†å™¨

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ token æ•°
        """
        self.max_context_tokens = max_context_tokens
        self.messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_prompt}
        ]
        # å½“å‰å®é™…ä½¿ç”¨çš„ token æ•°ï¼ˆä» API å“åº”è·å–ï¼‰
        self.current_tokens: int = 0

    def update_token_usage(self, prompt_tokens: int) -> None:
        """
        æ›´æ–° token ä½¿ç”¨é‡ï¼ˆä» API å“åº”è·å–ï¼‰

        Args:
            prompt_tokens: API è¿”å›çš„ prompt_tokens
        """
        self.current_tokens = prompt_tokens
        self._manage_context()

    def _manage_context(self) -> None:
        """ç®¡ç†ä¸Šä¸‹æ–‡ï¼Œå½“è¶…è¿‡é™åˆ¶æ—¶åˆ é™¤æ—§æ¶ˆæ¯ï¼ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼‰"""
        # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æœ€æ—§çš„éç³»ç»Ÿæ¶ˆæ¯
        while self.current_tokens > self.max_context_tokens and len(self.messages) > 1:
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ é™¤ç¬¬ä¸€ä¸ªéç³»ç»Ÿæ¶ˆæ¯
            removed_message = self.messages.pop(1)
            logger.debug(
                f"ä¸Šä¸‹æ–‡å·²æ»¡ï¼Œåˆ é™¤æ—§æ¶ˆæ¯ï¼Œå½“å‰ä½¿ç”¨: {self.current_tokens}/{self.max_context_tokens}"
            )
            # æ³¨æ„ï¼šåˆ é™¤æ¶ˆæ¯åï¼Œä¸‹æ¬¡ API è°ƒç”¨æ—¶ä¼šé‡æ–°è®¡ç®— token æ•°
            # è¿™é‡Œæˆ‘ä»¬æš‚æ—¶ä¿æŒ current_tokens ä¸å˜ï¼Œç­‰å¾…ä¸‹æ¬¡ API å“åº”æ›´æ–°

    def add_user_message(self, content: str) -> None:
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯"""
        self.messages.append({"role": "user", "content": f"{content}"})

    def add_assistant_content(self, content: str) -> None:
        """æ·»åŠ åŠ©æ‰‹å†…å®¹"""
        self.messages.append({"role": "assistant", "content": f"{content}"})

    def add_assistant_tool_call_result(self, tool_call_id: str, content: str) -> None:
        """æ·»åŠ åŠ©æ‰‹å·¥å…·è°ƒç”¨ç»“æœ"""
        self.messages.append(
            {"role": "tool", "tool_call_id": tool_call_id, "content": f"{content}"}
        )

    def add_assistant_tool_call(
        self, tool_call_id: str, name: str, arguments: str = ""
    ) -> None:
        """æ·»åŠ åŠ©æ‰‹å·¥å…·è°ƒç”¨"""
        self.messages.append(
            {
                "role": "assistant",
                "tool_calls": [
                    {
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": name,
                            "arguments": arguments,
                        },
                    }
                ],
            }
        )

    def get_messages(self) -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰æ¶ˆæ¯"""
        return self.messages.copy()

    def get_token_usage_percent(self) -> float:
        """
        è·å–å½“å‰ token ä½¿ç”¨ç™¾åˆ†æ¯”

        Returns:
            ä½¿ç”¨ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰
        """
        return (self.current_tokens / self.max_context_tokens) * 100

    def get_remaining_tokens(self) -> int:
        """
        è·å–å‰©ä½™å¯ç”¨ token æ•°

        Returns:
            å‰©ä½™ token æ•°
        """
        return max(0, self.max_context_tokens - self.current_tokens)


class ReActAgent:
    """ReAct Agent"""

    def __init__(self):
        """åˆå§‹åŒ– Agent"""
        # ç¦ç”¨ OpenAI å®¢æˆ·ç«¯çš„ HTTP æ—¥å¿—è¾“å‡º
        import httpx
        import logging
        
        # ç¦ç”¨ httpx çš„æ—¥å¿—
        httpx_logger = logging.getLogger("httpx")
        httpx_logger.setLevel(logging.WARNING)
        
        # ç¦ç”¨ httpcore çš„æ—¥å¿—ï¼ˆhttpx çš„åº•å±‚åº“ï¼‰
        httpcore_logger = logging.getLogger("httpcore")
        httpcore_logger.setLevel(logging.WARNING)
        
        self.client = OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
        )
        self.tools = self._create_tools()
        self.tool_executor = create_tool_executor(self.tools)
        self.message_manager = MessageManager(
            self._get_system_prompt(), config.max_context_tokens
        )
        # åˆå§‹åŒ–ä»»åŠ¡è§„åˆ’å™¨
        available_tool_names = [tool.name for tool in self.tools]
        self.task_planner = TaskPlanner(self.client, available_tool_names)
        self.current_plan: Optional[TaskPlan] = None  # å½“å‰ä»»åŠ¡è®¡åˆ’
        self.enable_planning: bool = config.enable_task_planning  # æ˜¯å¦å¯ç”¨è§„åˆ’åŠŸèƒ½
        self.chat_count = 0
        self.should_stop = False  # ä¸­æ–­æ ‡å¿—

    def _create_tools(self) -> List[Tool]:
        """åˆ›å»ºå·¥å…·åˆ—è¡¨"""
        return [
            ReadFileTool(config.work_dir),
            WriteFileTool(config.work_dir),
            DeleteFileTool(config.work_dir),
            CreateFileTool(config.work_dir),
            RenameFileTool(config.work_dir),
            ListFilesTool(config.work_dir),
            TreeFilesTool(config.work_dir),
            CreateFolderTool(config.work_dir),
            EditFileTool(config.work_dir),
            RunCommandTool(config.work_dir, config.command_timeout),
            SearchInFilesTool(config.work_dir, config.max_search_results),
            FindFilesTool(config.work_dir, config.max_find_files),
            DeleteFolderTool(config.work_dir),
            MoveFileTool(config.work_dir),
            CopyFileTool(config.work_dir),
            GitStatusTool(config.work_dir),
            GitDiffTool(config.work_dir),
            GitCommitTool(config.work_dir),
            GitBranchTool(config.work_dir),
            GitLogTool(config.work_dir),
        ]
        
    def _get_system_prompt_by_en(self) -> str:
        """Generate system prompt"""
        return f"""
You are a professional task-execution AI Agent.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€Core Responsibilitiesã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Accurately understand the user's true goal, not just the surface-level question
2. Follow the execution plan if one is provided, or decompose complex tasks into executable steps
3. Complete tasks within the constraints of the current environment
4. If a task fails, analyze the cause and attempt corrective solutions
5. Stop only after confirming the task is completed

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€Execution Principlesã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- Prioritize execution over explanation
- If an execution plan is provided, follow it step by step
- Think through the overall plan first, then execute step by step
- Evaluate each step by whether it moves closer to the goal
- When uncertain, attempt the Minimum Viable Action (MVP)
- Do not fabricate non-existent files, commands, or results
- Report progress as you complete each step of the plan

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€Environment Informationã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Operating System: {config.operating_system}
Working Directory: {config.work_dir}
Current Time (Beijing Time): {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
User Language Preference: {config.user_language_preference}

You must reason and act strictly based on the above real environment.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€Output Requirementsã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- Output only content that is valuable to the user
- Clearly state "Task completed" after the task is finished
- If the task cannot be completed, clearly explain the reason and suggest next steps

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€Prohibited Actionsã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- Do not assume the existence of unspecified tools or files
- Do not claim task completion without verification
- Do not output irrelevant or verbose explanatory content
"""


    def _get_system_prompt_by_cn(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿæç¤ºè¯"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡æ‰§è¡Œå‹ AI Agentã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æ ¸å¿ƒèŒè´£ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. å‡†ç¡®ç†è§£ç”¨æˆ·çš„ç›®æ ‡ï¼Œè€Œä¸ä»…æ˜¯è¡¨é¢é—®é¢˜
2. å¦‚æœæä¾›äº†æ‰§è¡Œè®¡åˆ’ï¼Œè¯·æŒ‰ç…§è®¡åˆ’é€æ­¥æ‰§è¡Œï¼›å¦åˆ™å°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤
3. åœ¨å½“å‰ç¯å¢ƒçº¦æŸä¸‹å®Œæˆä»»åŠ¡
4. å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œåˆ†æåŸå› å¹¶å°è¯•ä¿®æ­£æ–¹æ¡ˆ
5. åœ¨ç¡®è®¤ä»»åŠ¡å®Œæˆåæ‰åœæ­¢

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æ‰§è¡ŒåŸåˆ™ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- ä¼˜å…ˆæ‰§è¡Œï¼Œè€Œä¸æ˜¯è§£é‡Š
- å¦‚æœæä¾›äº†æ‰§è¡Œè®¡åˆ’ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è®¡åˆ’æ‰§è¡Œ
- å…ˆæ€è€ƒæ•´ä½“æ–¹æ¡ˆï¼Œå†é€æ­¥æ‰§è¡Œ
- æ¯ä¸€æ­¥éƒ½ä»¥"æ˜¯å¦æ›´æ¥è¿‘ç›®æ ‡"ä¸ºåˆ¤æ–­æ ‡å‡†
- ä¸ç¡®å®šæ—¶ï¼Œåšæœ€å°å¯è¡Œå°è¯•ï¼ˆMVPï¼‰
- ä¸ç¼–é€ ä¸å­˜åœ¨çš„æ–‡ä»¶ã€å‘½ä»¤æˆ–ç»“æœ
- å®Œæˆæ¯ä¸ªæ­¥éª¤åæŠ¥å‘Šè¿›åº¦

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ç¯å¢ƒä¿¡æ¯ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ“ä½œç³»ç»Ÿï¼š{config.operating_system}
å·¥ä½œç›®å½•ï¼š{config.work_dir}
å½“å‰æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ç”¨æˆ·è¯­è¨€åå¥½ï¼š{config.user_language_preference}

ä½ å¿…é¡»åŸºäºä»¥ä¸ŠçœŸå®ç¯å¢ƒè¿›è¡Œæ¨ç†ä¸è¡ŒåŠ¨ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€è¾“å‡ºè¦æ±‚ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- åªè¾“å‡ºå¯¹ç”¨æˆ·æœ‰ä»·å€¼çš„å†…å®¹
- ä»»åŠ¡å®Œæˆåï¼Œæ˜ç¡®è¯´æ˜â€œä»»åŠ¡å·²å®Œæˆâ€
- å¦‚æ— æ³•å®Œæˆï¼Œæ˜ç¡®è¯´æ˜åŸå› ä¸ä¸‹ä¸€æ­¥å»ºè®®

â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ç¦æ­¢äº‹é¡¹ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- ä¸è¦å‡è®¾ç¯å¢ƒä¸­å­˜åœ¨æœªè¯´æ˜çš„å·¥å…·æˆ–æ–‡ä»¶
- ä¸è¦åœ¨æœªéªŒè¯å‰å£°ç§°ä»»åŠ¡å·²å®Œæˆ
- ä¸è¦è¾“å‡ºæ— å…³çš„è§£é‡Šæ€§åºŸè¯
"""

    def _get_system_prompt(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿæç¤ºè¯"""
        return self._get_system_prompt_by_en()

    def _get_tools(self) -> List[Dict[str, Any]]:
        """è·å–å·¥å…·åˆ—è¡¨"""
        return [{"type": "function", "function": tool.to_dict()} for tool in self.tools]

    def stop_chat(self) -> None:
        """åœæ­¢å½“å‰å¯¹è¯"""
        self.should_stop = True
    
    def set_planning_enabled(self, enabled: bool) -> None:
        """è®¾ç½®æ˜¯å¦å¯ç”¨è§„åˆ’åŠŸèƒ½"""
        self.enable_planning = enabled
    
    def _should_create_plan(self, task_message: str, plan_status_callback: Optional[Callable[[str], None]] = None) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ›å»ºè®¡åˆ’ï¼ˆä½¿ç”¨ LLM æ™ºèƒ½åˆ¤æ–­ï¼‰
        
        Args:
            task_message: ä»»åŠ¡æ¶ˆæ¯
            plan_status_callback: å¯é€‰çš„è§„åˆ’çŠ¶æ€å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–° header æ˜¾ç¤º
            
        Returns:
            (æ˜¯å¦éœ€è¦è§„åˆ’, åˆ¤æ–­åŸå› )
        """
        if not self.enable_planning:
            return False, "è§„åˆ’åŠŸèƒ½å·²ç¦ç”¨"
        
        # å¦‚æœå·²ç»æœ‰è®¡åˆ’åœ¨æ‰§è¡Œï¼Œä¸åˆ›å»ºæ–°è®¡åˆ’
        if self.current_plan and self.current_plan.get_progress()["completed"] < len(self.current_plan.steps):
            return False, "å·²æœ‰è®¡åˆ’æ­£åœ¨æ‰§è¡Œä¸­"
        
        # æ¸…ç†æ¶ˆæ¯ï¼Œå»é™¤é¦–å°¾ç©ºç™½
        message = task_message.strip()
        
        # ç©ºæ¶ˆæ¯ä¸éœ€è¦è§„åˆ’
        if not message:
            return False, "æ¶ˆæ¯ä¸ºç©º"
        
        # ä½¿ç”¨ LLM æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦è§„åˆ’ï¼ˆå®Œå…¨äº¤ç»™æ¨¡å‹åˆ¤æ–­ï¼Œä¸é¢„è®¾è§„åˆ™ï¼‰
        try:
            if plan_status_callback:
                plan_status_callback("ğŸ” åˆ¤æ–­æ˜¯å¦éœ€è¦è§„åˆ’...")
            
            # æ„å»ºæ ‡å‡†çš„åˆ¤æ–­æç¤ºè¯ï¼ˆå‚è€ƒ OpenAI/Anthropic æœ€ä½³å®è·µï¼‰
            system_prompt = """You are a task analysis assistant. Your role is to determine whether a user's request requires detailed task planning before execution.

Task planning is needed when:
- The request requires using tools (file operations, command execution, Git operations, etc.)
- The request involves multiple steps or complex workflows
- The request needs to be broken down into smaller actionable steps

Task planning is NOT needed when:
- The request is a simple greeting or expression of gratitude
- The request is a straightforward knowledge question that can be answered directly
- The request is a simple informational query

Respond with only "yes" or "no" followed by a brief reason in parentheses."""

            user_prompt = f"""Analyze the following user request and determine if it requires detailed task planning:

User request: "{message}"

Respond with: "yes (reason)" or "no (reason)"."""

            # ä½¿ç”¨æµå¼è¾“å‡º
            stream_response = self.client.chat.completions.create(
                model=config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.1,  # Very low temperature for deterministic classification
                max_tokens=50,  # Allow space for brief reason
                stream=True,
            )
            
            result = ""
            try:
                for chunk in stream_response:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, "content") and delta.content:
                            result += delta.content
                            if plan_status_callback:
                                plan_status_callback(f"ğŸ” åˆ¤æ–­ä¸­: {result[:30]}...")
            finally:
                try:
                    stream_response.close()
                except:
                    pass
            
            result = result.strip()
            result_lower = result.lower()
            
            # è§£æç»“æœï¼šæå– yes/no å’ŒåŸå› 
            needs_planning = any(result_lower.startswith(prefix) for prefix in ["yes", "y"])
            
            # æå–åŸå› ï¼ˆå¦‚æœæœ‰ï¼‰
            reason = "LLMåˆ¤æ–­"
            if "(" in result and ")" in result:
                try:
                    reason = result.split("(")[1].split(")")[0].strip()
                except:
                    pass
            
            logger.debug(f"è§„åˆ’åˆ¤æ–­: '{message}' -> {needs_planning} (åŸå› : {reason}, LLMå›ç­”: {result})")
            return needs_planning, reason
            
        except Exception as e:
            logger.warning(f"è§„åˆ’åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤ä¸è§„åˆ’")
            if plan_status_callback:
                plan_status_callback(f"âš ï¸ åˆ¤æ–­å¤±è´¥")
            return False, f"åˆ¤æ–­å¤±è´¥: {str(e)}"
    
    def chat(self, task_message: str, output_callback: Optional[Callable[[str, bool], None]] = None, plan_status_callback: Optional[Callable[[str], None]] = None) -> None:
        """
        å¤„ç†ç”¨æˆ·ä»»åŠ¡

        Args:
            task_message: ç”¨æˆ·ä»»åŠ¡æ¶ˆæ¯
            output_callback: å¯é€‰çš„è¾“å‡ºå›è°ƒå‡½æ•°ï¼Œæ¥å— (text, end_newline) å‚æ•°
                            å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨å›è°ƒè€Œä¸æ˜¯ print
            plan_status_callback: å¯é€‰çš„è§„åˆ’çŠ¶æ€å›è°ƒå‡½æ•°ï¼Œæ¥å— (status_text) å‚æ•°
                                 ç”¨äºæ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€æ˜¾ç¤º
        """
        # é‡ç½®ä¸­æ–­æ ‡å¿—
        self.should_stop = False
        
        # å®šä¹‰è¾“å‡ºå‡½æ•°
        def output(text: str, end_newline: bool = True):
            if output_callback:
                output_callback(text, end_newline)
            else:
                print(text, end="\n" if end_newline else "", flush=True)
        
        # å®šä¹‰è§„åˆ’çŠ¶æ€æ›´æ–°å‡½æ•°
        def update_plan_status(status: str):
            if plan_status_callback:
                plan_status_callback(status)
        
        # ä»»åŠ¡è§„åˆ’é˜¶æ®µ - æ˜¾ç¤ºåˆ¤æ–­ç»“æœ
        needs_planning, _reason = self._should_create_plan(task_message, update_plan_status)
        
        if needs_planning:
            update_plan_status("ğŸ“‹ åˆ†æä»»åŠ¡ä¸­...")
            
            try:
                self.current_plan = self.task_planner.create_plan(task_message, update_plan_status)
                
                # æ›´æ–°è§„åˆ’çŠ¶æ€ä¸ºè¿›åº¦æ˜¾ç¤º
                progress = self.current_plan.get_progress()
                update_plan_status(f"ğŸ“‹ è®¡åˆ’å®Œæˆ ({len(self.current_plan.steps)} æ­¥) | è¿›åº¦: {progress['completed']}/{progress['total']}")
                
                # å°†è®¡åˆ’æ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œè®©æ¨¡å‹çŸ¥é“è®¡åˆ’
                plan_summary = f"\næ‰§è¡Œè®¡åˆ’ï¼ˆå…± {len(self.current_plan.steps)} æ­¥ï¼‰ï¼š\n"
                for step in self.current_plan.steps:
                    plan_summary += f"{step.step_number}. {step.description}\n"
                task_message = f"{task_message}\n\n{plan_summary}"
                
            except Exception as e:
                logger.error(f"è§„åˆ’å¤±è´¥: {e}")
                update_plan_status(f"âš ï¸ è§„åˆ’å¤±è´¥: {str(e)[:30]}")
                self.current_plan = None
        else:
            logger.debug(f"ç›´æ¥æ‰§è¡Œä»»åŠ¡: {task_message}")
            # æ¸…é™¤è§„åˆ’çŠ¶æ€
            update_plan_status("")
        
        self.message_manager.add_user_message(task_message)
        while True:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­ï¼ˆåœ¨ä¸»å¾ªç¯å¼€å§‹æ—¶ï¼‰
            if self.should_stop:
                logger.info("å¯¹è¯åœ¨ä¸»å¾ªç¯è¢«ç”¨æˆ·ä¸­æ–­")
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯è¯´æ˜ç”¨æˆ·ä¸­æ–­äº†å¯¹è¯
                self.message_manager.messages.append({
                    "role": "system",
                    "content": "[ç”¨æˆ·åœ¨å¯¹è¯å¼€å§‹å‰ä¸­æ–­äº†ä»»åŠ¡]"
                })
                output("\n\n[å¯¹è¯å·²è¢«ç”¨æˆ·ä¸­æ–­]", end_newline=True)
                break
            self.chat_count += 1

            logger.debug(f"=== Chat Round {self.chat_count} ===")
            logger.debug(
                f"Messages: {json.dumps(self.message_manager.get_messages(), indent=2, ensure_ascii=False)}"
            )

            # è°ƒç”¨ APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            max_retries = 3
            retry_count = 0

            while retry_count < max_retries:
                try:
                    stream_response: Stream[ChatCompletionChunk] = (
                        self.client.chat.completions.create(
                            model=config.model,
                            messages=self.message_manager.get_messages(),
                            stream=True,
                            temperature=1,
                            top_p=1,
                            max_tokens=4096,
                            tools=self._get_tools(),
                            tool_choice="auto",
                        )
                    )
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    retry_count += 1
                    logger.error(f"API è°ƒç”¨å¤±è´¥: {e}")
                    raise

            else:
                # é‡è¯•æ¬¡æ•°ç”¨å°½
                logger.error("API è°ƒç”¨å¤±è´¥: å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                error_msg = "\n=== é”™è¯¯ä¿¡æ¯ ===\nAPI è°ƒç”¨å¤±è´¥: å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°\n=== é”™è¯¯ä¿¡æ¯ç»“æŸ ===\n"
                output(error_msg, end_newline=True)
                return  # ä¼˜é›…é€€å‡ºï¼Œä¸æŠ›å‡ºå¼‚å¸¸

            # å¤„ç†æµå¼å“åº”
            content = ""
            last_tool_call_id = None
            tool_call_acc = {}
            usage = None
            
            start_reasoning_content = False
            start_content = False
            start_tool_call = False
            
            # å®šä¹‰è¾“å‡ºå‡½æ•°ï¼ˆå·²åœ¨æ–¹æ³•å¼€å§‹å¤„å®šä¹‰ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å®šä¹‰ï¼‰

            try:
                for chunk in stream_response:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
                    if self.should_stop:
                        logger.info("æµå¼å“åº”è¢«ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­æµ...")
                        stream_response.close()  # å…³é—­æµï¼Œåœæ­¢åç«¯ç»§ç»­ç”Ÿæˆ
                        break
                    
                    # è·å– usage ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­ï¼‰
                    if hasattr(chunk, "usage") and chunk.usage is not None:
                        usage = chunk.usage

                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta

                        if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                            if not start_reasoning_content:
                                output(f"\n{'='*config.log_separator_length} æ¨¡å‹æ€è€ƒ {'='*config.log_separator_length}\n")
                                start_reasoning_content = True
                            output(delta.reasoning_content, end_newline=False)

                        if hasattr(delta, "content") and delta.content:
                            if not start_content:
                                output(f"\n{'='*config.log_separator_length} æœ€ç»ˆå›å¤ {'='*config.log_separator_length}\n")
                                start_content = True
                            chunk_content = delta.content
                            content += chunk_content
                            output(chunk_content, end_newline=False)

                        if hasattr(delta, "tool_calls") and delta.tool_calls:
                            if not start_tool_call:
                                output(f"\n{'='*config.log_separator_length} å·¥å…·è°ƒç”¨ {'='*config.log_separator_length}\n")
                                start_tool_call = True
                            for tc in delta.tool_calls:
                                tc_id = tc.id or last_tool_call_id

                                if tc_id is None:
                                    # è¿ç¬¬ä¸€ä¸ª id éƒ½æ²¡æœ‰ï¼Œç›´æ¥è·³è¿‡ï¼ˆæå°‘è§ï¼‰
                                    continue

                                last_tool_call_id = tc_id

                                if tc_id not in tool_call_acc:
                                    tool_call_acc[tc_id] = {
                                        "id": tc_id,
                                        "name": "",
                                        "arguments": "",
                                    }

                                # æ‹¼ nameï¼ˆè™½ç„¶ä¸€èˆ¬åªæ¥ä¸€æ¬¡ï¼Œä½†è§„èŒƒå…è®¸æ‹†ï¼‰
                                if tc.function:
                                    if tc.function.name:
                                        tool_call_acc[tc_id]["name"] += tc.function.name
                                        output(tc.function.name, end_newline=False)
                                    if tc.function.arguments:
                                        tool_call_acc[tc_id][
                                            "arguments"
                                        ] += tc.function.arguments
                                        output(tc.function.arguments, end_newline=False)
            except Exception as e:
                # å¦‚æœåœ¨å¤„ç†æµæ—¶å‘ç”Ÿå¼‚å¸¸ï¼ˆåŒ…æ‹¬å…³é—­æµï¼‰ï¼Œè®°å½•æ—¥å¿—
                logger.debug(f"æµå¤„ç†å¼‚å¸¸: {e}")
                # å¦‚æœæ˜¯ç”¨æˆ·ä¸­æ–­ï¼Œä¸éœ€è¦æŠ›å‡ºå¼‚å¸¸
                if not self.should_stop:
                    raise
            finally:
                # ç¡®ä¿æµè¢«å…³é—­
                try:
                    stream_response.close()
                except Exception:
                    pass
            
            # å¦‚æœç”¨æˆ·ä¸­æ–­äº†å¯¹è¯ï¼Œå°†ä¸­æ–­ä¿¡æ¯æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            if self.should_stop:
                # å¦‚æœæœ‰éƒ¨åˆ†å†…å®¹ï¼Œå…ˆä¿å­˜
                if content.strip():
                    self.message_manager.add_assistant_content(content)
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯è¯´æ˜ç”¨æˆ·ä¸­æ–­äº†å¯¹è¯
                self.message_manager.messages.append({
                    "role": "system",
                    "content": "[ç”¨æˆ·åœ¨æ­¤å¤„ä¸­æ–­äº†å¯¹è¯ï¼Œæœªå®Œæˆçš„ä»»åŠ¡å·²æš‚åœ]"
                })
                logger.info("å·²å°†ç”¨æˆ·ä¸­æ–­ä¿¡æ¯æ·»åŠ åˆ°ä¸Šä¸‹æ–‡")
                break

            # æ›´æ–° token ä½¿ç”¨é‡ï¼ˆä» API å“åº”è·å–ï¼‰
            if usage:
                prompt_tokens = getattr(usage, "prompt_tokens", None)
                if prompt_tokens is not None:
                    self.message_manager.update_token_usage(prompt_tokens)
                    completion_tokens = getattr(usage, "completion_tokens", 0)
                    total_tokens = getattr(usage, "total_tokens", 0)
                    logger.debug(
                        f"\nToken ä½¿ç”¨: prompt={prompt_tokens}, completion={completion_tokens}, total={total_tokens}"
                    )
                else:
                    logger.warning("\nAPI å“åº”ä¸­æœªæ‰¾åˆ° prompt_tokens")
            else:
                logger.warning("\næµå¼å“åº”ä¸­æœªæ‰¾åˆ° usage ä¿¡æ¯")

            if tool_call_acc:
                # æ›´æ–°å½“å‰æ­¥éª¤çŠ¶æ€ï¼ˆå¦‚æœæœ‰è®¡åˆ’ï¼‰
                current_step = None
                if self.current_plan:
                    current_step = self.current_plan.get_current_step()
                    if current_step and current_step.status == StepStatus.PENDING:
                        current_step.mark_started()
                        progress = self.current_plan.get_progress()
                        # æ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€
                        update_plan_status(f"ğŸ“‹ æ‰§è¡Œä¸­: {progress['completed']}/{progress['total']} ({progress['progress_percent']:.0f}%) | æ­¥éª¤ {current_step.step_number}")
                
                for tc_id, tc_data in tool_call_acc.items():
                    # logger.info(f"=== Tool Call ===")
                    # logger.debug(f"name: {tc_data['name']}")
                    # logger.debug(f"arguments: {tc_data['arguments']}")
                    self.message_manager.add_assistant_tool_call(
                        tc_id, tc_data["name"], tc_data["arguments"]
                    )
                    tool_call_result = self.tool_executor.execute(
                        tc_data["name"], tc_data["arguments"]
                    )
                    result_content = None
                    # å¤„ç†æ ‡å‡†åŒ–çš„è¿”å›æ ¼å¼
                    if isinstance(tool_call_result, dict):
                        result_content = json.dumps(tool_call_result, ensure_ascii=False, indent=2)
                        # æ£€æŸ¥å·¥å…·æ‰§è¡Œæ˜¯å¦æˆåŠŸ
                        is_success = tool_call_result.get("success", False)
                        tool_result = tool_call_result.get("result", "")
                        tool_error = tool_call_result.get("error")
                    else:
                        # å…¼å®¹æ—§çš„è¿”å›æ ¼å¼
                        result_content = tool_call_result
                        is_success = True  # å‡è®¾æˆåŠŸ
                        tool_result = tool_call_result
                        tool_error = None
                    
                    # æ›´æ–°æ­¥éª¤çŠ¶æ€
                    if self.current_plan and current_step:
                        if is_success:
                            # æˆªæ–­è¿‡é•¿çš„ç»“æœ
                            result_summary = str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result)
                            current_step.mark_completed(result_summary)
                            # æ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€
                            progress = self.current_plan.get_progress()
                            update_plan_status(f"ğŸ“‹ æ‰§è¡Œä¸­: {progress['completed']}/{progress['total']} ({progress['progress_percent']:.0f}%)")
                        else:
                            current_step.mark_failed(tool_error or "å·¥å…·æ‰§è¡Œå¤±è´¥")
                            # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ï¼‰
                            self.current_plan.move_to_next_step()
                            # æ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€
                            progress = self.current_plan.get_progress()
                            update_plan_status(f"ğŸ“‹ æ‰§è¡Œä¸­: {progress['completed']}/{progress['total']} ({progress['progress_percent']:.0f}%) âš ï¸")
                    
                    self.message_manager.add_assistant_tool_call_result(
                        tc_data["id"], result_content
                    )
                
                # å¦‚æœå½“å‰æ­¥éª¤å®Œæˆï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
                if self.current_plan and current_step and current_step.status == StepStatus.COMPLETED:
                    self.current_plan.move_to_next_step()
                    # æ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€
                    progress = self.current_plan.get_progress()
                    update_plan_status(f"ğŸ“‹ æ‰§è¡Œä¸­: {progress['completed']}/{progress['total']} ({progress['progress_percent']:.0f}%)")
                
                continue
            else:
                # æœ€ç»ˆå›å¤é˜¶æ®µ
                # logger.info(f"=== Final Answer ===")
                # logger.info(content)
                
                # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæ›´æ–°è®¡åˆ’çŠ¶æ€
                if self.current_plan:
                    # æ ‡è®°æ‰€æœ‰å‰©ä½™æ­¥éª¤ä¸ºå®Œæˆï¼ˆå¦‚æœä»»åŠ¡å·²å®Œæˆï¼‰
                    progress = self.current_plan.get_progress()
                    if progress["pending"] > 0:
                        # å¦‚æœè¿˜æœ‰å¾…æ‰§è¡Œçš„æ­¥éª¤ï¼Œæ ‡è®°ä¸ºè·³è¿‡ï¼ˆå¯èƒ½è®¡åˆ’è¿‡äºè¯¦ç»†ï¼‰
                        for step in self.current_plan.steps:
                            if step.status == StepStatus.PENDING:
                                step.mark_skipped("ä»»åŠ¡å·²å®Œæˆï¼Œæ­¥éª¤è‡ªåŠ¨è·³è¿‡")
                    
                    # æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
                    final_progress = self.current_plan.get_progress()
                    if final_progress["total"] > 0:
                        # æ›´æ–° header ä¸­çš„è§„åˆ’çŠ¶æ€
                        status_text = f"âœ… å®Œæˆ: {final_progress['completed']}/{final_progress['total']} ({final_progress['progress_percent']:.0f}%)"
                        if final_progress["failed"] > 0:
                            status_text += f" âš ï¸{final_progress['failed']}"
                        update_plan_status(status_text)
                
                self.message_manager.add_assistant_content(content)
                break
